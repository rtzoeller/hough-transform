\documentclass[letterpaper,12pt,titlepage]{article}

\title{Line detection via the Hough transform}
\date{December 23, 2015}
\author{Ryan Zoeller}

\begin{document}
\maketitle
\newpage

\section*{Abstract}
The classical Hough transform provides a convenient technique for detecting lines in an image.
A thresholded edge matrix is generated for the image (using any one of a number of edge detection
operators, e.g. the Sobel operator); this matrix is then used in a voting process carried
out in a parameter space. In the classical Hough transform this parameter space is the polar
normal form of a line, which is especially useful as it allows for the constrained
search space $\theta \in (-\pi,\pi]$ as well as for representation of vertical lines.
This paper presents a multithreaded implementation of the classical Hough transform, which assigns
each thread a sub-interval of the search space. Due to the partitioning of the vote accumulation
matrix, each thread can operate on this sub-interval without synchronization, resulting in a dramatic
performance increase on dense inputs.

\section{Introduction}
In the generalized (as well as classical) Hough transform, an edge matrix is generated
for the input image through two-dimensional image convolution. In practice, a variety of image
kernels are used for this convolution, including the Sobel, Scharr and Prewitt operators.
The edge matrix is then thresholded, producing a set of non-zero coordinate pairs.
These coordinate pairs are the input to the accumulation procedure used to detect prominent
features -- which, in the case of the classical Hough transform, are lines.
\\
In the classical Hough transform, the parameter space used for vote accumulation represents
the polar normal form of a line, which is given by $\rho=x*\cos{\theta}+y*\sin{\theta}$.
It has several advantages over the traditional point-slope equations, namely that the search
space is bounded. In the traditional point-slope form, the slope may approach both positive
and negative infinity; the representation of vertical lines requires this. Instead, the
polar normal form allows the entire range of theta to be swept using some small increment.

\section{Challenges}
While using a polar normal representation as the vote accumulation space is
conceptually better than the using point-slope form, the implementation of
the Hough transform in general presents some challenges. Most significant is
the high computational cost of the algorithm, which is in many ways hidden by
the algorithm's asymptotic complexity, as well as the direct correlation between
numerical accuracy and the constant factor abstracted by this complexity in
big $\mathcal{O}$ notation.

\subsection{Asymptotic Complexity}
The cost of computing the edge matrix using image convolution is $\mathcal{O}(mn)$ for
an image of dimensions $m$ by $n$. For a square image, this is equivalent to $\mathcal{O}(n^{2})$.
The cost of convolving each element is linear with the number of entries in the kernel, however
this cost is dominated by the cost of convolution as the kernel is required to be smaller than
the input image. Finding the entries of the edge matrix which satisfy some threshold is also linear with
the number of elements in the input image (i.e. $\mathcal{O}(mn)$ in the general case of an $m$
by $n$ image).
\\
The majority of the computational cost of the Hough transform instead stems from the
$\mathcal{O}(n)$ search of the parameter space, which is simply linear with the number of
thresholded points. While this is asymptotically dominated by the image convolution cost,
in practice the increment used to search the range of theta (or any other parameterized value)
is small, leading to a high constant factor. In the presented implementation, 1000 values of
theta are sampled for each candidate point.

\subsection{Numerical Precision}
The accuracy of the computed feature (in the classical case, a line) is proportional to the
precision of the discretized parameter space. Unfortunately, the computational cost is also
directly proportional to this precision. As a result, it is extremely important to determine
the required precision before generating the accumulation matrix, to avoid a result outside
the required tolerance or a result which is excessively expensive to compute.
\\
A further challenge the generalized Hough transform faces is the limit of floating point
precision. As the precision of the discretized parameter space increases, the impact of
numerical error also increases. Since the accumulation matrix is used to directly calculate
the most prominent features, these numerical errors can distort the actual results. At some
precision (before the machine epsilon is reached), the result is no longer meaningful, as
numerical error will contribute to neighboring vote buckets instead of the true vote bucket.
The result of this is either that the actual value will not statistically stand out
(i.e. neighboring buckets will have approximately the same value) or that a neighboring value
will succeed the actual value. One potential solution to this is to 'smear' the votes over a
region, i.e. to spread the vote across multiple buckets in a weighted fashion. This feature
is not present in the presented implementation however, due to performance and weighting concerns.

\end{document}
